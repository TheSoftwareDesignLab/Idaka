GP1;The goal of the model that is going to be trained should be considered when selecting the most appropriate metrics for assessing the model performance.
GP2;It is important to identify the needs/requirements of model retraining.
GP3;If an ML model is published as a cloud service, specifically, when a classification/prediction task uses ML cloud-based services, it is important to define the use case and the model requirements in order to identify how frequently the service should be invoked.
GP4;When using probabilistic forecasting in a decision system, it is necessary to decouple the probabilistic model optimization from the probability threshold selection.
GP5;The distribution of the training data should reflect the real distribution.
GP6;"If a model is expected to detect ""something"" in an image, then representative examples of that ""something"" should be present in the training and testing data."
GP7;When measuring a dataset size it should not be done only by referring to storage space but also in terms of rows and columns.
GP8;If it is required to augment the number of instances in the negative class, preexisting datasets could be used for including more instances in the dataset.
GP9;The images that are going to be used as instances of the negative class should have some common characteristics with the positive ones.
GP10;The minimum size of the object that is going to be detected should be present in the data that is going to be used for training the model.
GP11;The object region of interest (ROI) should have a similar aspect ratio in all the positive images.
GP12;For each possible attribute/column in a dataset, we should identify its type before applying any pre-processing technique.
GP13;It should be determined what variables are dependent and what ones are independent.
GP14;When analyzing time series/temporal data, possible temporal trends should be identified.
GP15;When analyzing time series/temporal data, weird trends should be identified.
GP16;It is needed to identify missing values and their nature before doing any pre-processing.
GP17;Before applying any pre-processing technique, potential errors in data should be identified.
GP18;If correlation between existing features and instances of geographical data are needed, then, geographical data should be clustered.
GP19;For improving model performance with a dataset that has a categorical feature, the data should be split by each category.
GP20;If it is needed to optimize hyper-parameters, the dataset should be split into training, validation and test sets in order to prevent overfitting and avoid biased computing metrics.
GP21;"In order to prevent overfitting and avoid biased metrics when optimizing, the whole dataset should be divided into trainand test; then, the training data should be divided into folds for cross-validation"
GP22;The size of a dataset should be only calcuated after the data pre-processing.
GP23;If an imputation technique is used, the same technique should be used in the training, validation and testing sets.
GP24;When having an unbalanced dataset, balancing the datashould be done only in the training dataset.
GP25;Oversampling should be done only after the whole dataset is split into train, validation and test sets.
GP26;When undersampling data, the samples that are being selected should be randomly sampled.
GP27;Transformation of numerical data should be done in all the sets by using the statistics computed on the training data.
GP28;Transformations should take into account (i) the model that is going to be used and (ii) the data nature.
GP29;Features/attributes should be scaled when using a support vector machine (SVM) model.
GP30;When transforming numerical data into ratios and the transformation leads to infinite values, smoothing should be used.
GP31;if an algorithm does not support categorical data, this type of feature should be encoded into multiple binary features or by counting frequencies.
GP32;Datetime data could be partitioned into their corresponding components (e.g., if you have day-month-year then split it into day, month, year).
GP33;Data augmentation should be used in order to prevent over-fitting.
GP34;For analyzing handwritten text images, the image should be split into individual characters.
GP35;If the task of a model is detecting objects, then each region of interest (ROI) should be cropped instead of using the complete image.
GP36;When handling geographical data and the latitude and longitude will be transformed, directional statistics should be used to deal with potential interdependence of latitude and longitude, and with scales wrapping around.
GP37;When handling geographical data, the geodesic distance should be used to compute the distance between two points of interest.
GP38;If a variable is dependable on time along with other features,then, the timeseries variable could be encoded as frequencies.
GP39;When encoding geographical data, Sine and Cosine facets could be used for representing the cyclical characteristics from date time features.
GP40;If noise is present in a dataset, then, it should be removed,with techniques such as smoothing.
GP41;If rectified linear unit (ReLU) is being used as an activation function in NeuralNetworks (NN), then, the features should be scaled.
GP42;If clustering is needed, the dataset contains aggregated structures (e.g., a 3D array), and the order of the aggregated data matters, then a new feature should be created in to encode the order.
GP43;When working with an long short-term memory (LSTM) network the sequences that are going to be fed into the NN should be padded in order to equalize the length of all the sequences.
GP44;When data is being anonymized with hashing, it should be done over a unique identifier using salt.
GP45;Manual data labeling could be expensive, so it could be done by more than one person in order to scale the labeling process.
GP46;In the case of object detection, when selecting the region of interest, a tool should be used to fix the aspect ratio for all the labeled objects.
GP47;Feature engineering should be only executed with the training data and without taking into account the whole dataset.
GP48;Multivariate feature engineering is preferred over univariate feature selection.
GP49;It is relevant to check existing techniques of dimensionality reduction.
GP50;When using feature selection algorithms, it is important to evaluate which are their assumptions and if they hold for a particular case.
GP51;When using the principal component analysis (PCA) method, its assumptions should be checked.
GP52;When selecting features, the existing techniques for this purpose should be checked.
GP53;When using Logistic Regression for selecting features, select the significant ones.
GP54;Regardless the classifier that is being used, mutual information techniques for feature selection should be considered.
GP55;Using backward and forward feature selection should be taken into account regardless the classifier that is being used.
GP56;For feature selection, the regularization techniques that are part of the algorithm should be considered (e.g., Lasso, Elastic Net when high collinearity).
GP57;The usage of Lasso, Ridge, or Glmnet or feature selection in a multiple regression model should be considered.
GP58;If the objective is to build specialized models, then a specific model for each category in a categorical feature should be trained.
GP59;If the dataset that is going to be used has multiple features in different scale systems, then invariant-algorithms like tree based algorithms should be used.
GP60;When training prediction models with time series data, then, sliding windows should be used.
GP61;Test data should not be included in the training data in order to have a fair metric.
GP62;As many data as possible should be used when training a model.
GP63;When training a neural network (NN), dropout layer(s) should be used to avoid overfitting.
GP64;If symmetry is needed to be avoided, in a neural network, their components should be initialized with different values.
GP65;Neural Networks weights should be initialized randomly to avoid symmetry.
GP66;When training a NN, Early Stopping should be used (e.g., after 10 epochs the model has not improved in 0.001 in accuracy).
GP67;If images are being used for training a neural network, it is preferable a convolutional layer than a fully connected one.
GP68;if a neural network stagnates with non-optimal results, then the learning rate should be decreased.
GP69;If there is not enough data when training a deep neural network from scratch, transfer learning with fine tuning should be tried.
GP70;If a deep neural network does not converge in an specific number of iterations, then increasing this number should be tried.
GP71;When having a large dataset that does not fit in memory, multiples models could be first trained on random data samples, and then an ensemble with those models could be created to get a more accurate model than a base learner with fewer data.
GP72;A model should be retrained with an online approximation if the model needs to dynamically adapt to new patterns from the data.
GP73;If dependencies between features exist, it is suggested to used a gradien boost decision tree (GBDT) instead of a Random Forest (RF).
GP74;The number of parameters of a deep neural network should be proportional to the number of instances needed for training a deep neural network.
GP75;If there is a large data variance, the number of layers in a neural network must be increased.
GP76;A neural network should be trained with different initialized weights several times, in order to first get an average of the results and its variance and then to get a more specific judgment of the overall performance.
GP77;If a neural netwok does not converge, then the learning rate should be reduced by a half or one third.
GP78;When a neural network does not converge, the size of the mini-batches should be increased.
GP79;For faster convergence of Multi Layer Perceptrons, Stochastic Gradient Descent (SGD) should be used.
GP80;If robustness is required in a machine learning model, then, Adversarial machine learning should be used.
GP81;If the time-performance of a model is being  affected due to a large time series data, then the data could be sub-sampled.
GP82;When sub-sampling time-series data, it should be checked that unwanted confounding between sampled origins and seasonality in data is not being introduced.
GP83;Before combining models, the correlation of models' prediction should be measured.
GP84;Only independent models should be combined in an ensemble.
GP85;When training a model with mixed type features, which includes time series features, separate models for each part of the representation should be built, and then the different models should be combined in an ensemble.
GP86;When using clustering, the model should be re-trained with new data to build more clusters and increase its efficiency.
GP87;In order to deal with concept drift when working with data streams, models should be re-trained with small a limited number of instances.
GP88;A model should be retrained in offline mode, due to this method could lead to a better global approximation.
GP89;In the case of online and offline re-training are not an option, then using batch/mini-batch approach should be used.
GP90;The optimization process should be done with a validation set.
GP91;Each model that is combined into an ensemble should be optimized independently.
GP92;In order to monitor the learning process's evolution, the model's predefined metrics should be calculated at the end of each epoch.
GP93;When data is insufficient to split it into train, validation and test, then cross-validation should be used.
GP94;It is preferable to use cross-validation instead of a training and test data split for training and testing a model.
GP95;It is preferable to use nested cross-validation instead of a training and test split for training and testing a model.
GP96;When training a neural network, in order to ensure reproducibility and fair comparison, the seed used for generating random numbers in the training process should be fixed.
GP97;In order to prevent overfitting, cross-validation should be used.
GP98;In order to avoid overfitting nested cross-validation should be used.
GP99;First tune the hyper-parameters of the probabilistic model and then adjust the decision thresholds.
GP100;In order to monitor the evolution of the learning process and identify issues at an early stage on it, learning curves should be plotted.
GP101;When evaluating a model, bootstrapping or cross-validation should be tried.
GP102;When training/validating/testing a model, the process of cross-validation should be repeated with different seeds to prevent the impact of using a fixed seed.
GP103;If you want to lower the uncertainty of the hyper-parameter tuning process, then nested cross-validation should be used.
GP104;In order to avoid overfitting the performance between training and testing should be compared.
GP105;Use adversarial inputs for testing a model in order to ensure robustness.
GP106;"After the best hyper-parameters are found, the model with those “optimum"" hyper-parameters should be tested in the set built for testing."
GP107;If a superset vocabulary was built for a natural language processing {NLP} task, then the model bias should be checked.
GP108;The comparison between models to identify the best model should be made in the test set.
GP109;Previously annotated data should be used when testing machine learning models with unit tests.
GP110;The performance and the time required to train a model should be taken into account when comparing models
GP111;The membership of the testing data to the training data should be tested.
GP112;When cross-validation is used for testing, then the hold-out set should not be used.
GP113;A deployed model should be the one that has the best hyper-parameters, but it should be re-trained with the entire dataset.
GP114;When deploying a model, the entire pipeline should be exported instead of exporting only the model.
GP115;After deployment, the new data that will serve as input for the model should be constantly monitored to detect any deviation from the original data.
GP116;When monitoring the data distribution, if it deviates from the original one used for training and tuning models, then the model should be retrained in order to avoid degradation.
GP117;For the data cleaning procedure stage, routines should be used to avoid implementing everything from scratch.
GP118;Across the ML stages, pipelines should be used to automate processes and save time in complex tasks.
GP119;In order to enable reproducibility in data pre-processing, pipelines should be used and exported.
GP120;For each trained model, a separate file should be used in order to keep track of all the possible experiments.
GP121;The (hyper-)parameters used in the training process should be documented.
GP122;Once a model is deployed and a petition of data deletion is made, the data should be deleted across the entire pipeline.
GP123;When dealing with large datasets or large files, aspects such as parallel executions, GPU usage, and input/output efficiency should be taken into account.
GP124;In order to optimize, parallel execution should be used if it supported by the algorithms.
GP125;It should be verified if both, model and dataset, fit in memory.
GP126;When dealing with large corpus, in NLP-related tasks, sparse structures should be used to improve the implementation performance.
GP127;When dealing with large data, resource-aware implementations should be used.
